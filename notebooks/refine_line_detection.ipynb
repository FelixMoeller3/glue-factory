{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4f83ba",
   "metadata": {},
   "source": [
    "## Oxparis Visualization Notebook\n",
    "In this notebook one can load images and gt from the oxparis dataset as given by the POLD2 team (they were givin it by remi).\n",
    "Possible actions in this Notebook:\n",
    "- Load Images from the dataset and look at it and GT\n",
    "- Run model on image and compare result with gt\n",
    "- also detect lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8067ff4d-70b1-4ff9-8612-98c0e2636380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/fmoeller/.local/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import flow_vis\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from gluefactory.datasets import get_dataset\n",
    "from gluefactory.models.deeplsd_inference import DeepLSD\n",
    "from omegaconf import OmegaConf\n",
    "from gluefactory.settings import DATA_PATH\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "\n",
    "line_neighborhood = 5 # in px used to nortmalize/ denormalize df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d816239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_lsd_model(device=\"cuda\"):\n",
    "    deeplsd_conf = {\n",
    "    \"detect_lines\": True,\n",
    "    \"line_detection_params\": {\n",
    "        \"merge\": True,\n",
    "        \"filtering\": True,\n",
    "        \"grad_thresh\": 3,\n",
    "        \"grad_nfa\": True,\n",
    "    },\n",
    "    \"weights\": \"DeepLSD/weights/deeplsd_md.tar\",  # path to the weights of the DeepLSD model (relative to DATA_PATH)\n",
    "    }\n",
    "    deeplsd_conf = OmegaConf.create(deeplsd_conf)\n",
    "\n",
    "    ckpt_path = DATA_PATH / deeplsd_conf.weights\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=device, weights_only=False)\n",
    "    deeplsd_net = DeepLSD(deeplsd_conf)\n",
    "    deeplsd_net.load_state_dict(ckpt[\"model\"])\n",
    "    deeplsd_net = deeplsd_net.to(device).eval()\n",
    "    return deeplsd_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4f96e",
   "metadata": {},
   "source": [
    "Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54af8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_vis(df, ang, line_neighborhood=5):\n",
    "    norm = line_neighborhood + 1 - np.clip(df, 0, line_neighborhood)\n",
    "    flow_uv = np.stack([norm * np.cos(ang), norm * np.sin(ang)], axis=-1)\n",
    "    flow_img = flow_vis.flow_to_color(flow_uv, convert_to_bgr=False)\n",
    "    return flow_img\n",
    "\n",
    "def normalize_df(df):\n",
    "    return -torch.log(df / line_neighborhood + 1e-6)\n",
    "\n",
    "def denormalize_df(df_norm):\n",
    "    return torch.exp(-df_norm) * line_neighborhood\n",
    "\n",
    "def visualize_img_and_pred(keypoints,heatmap,distance_field,angle_field,img):\n",
    "    _, ax = plt.subplots(1, 4, figsize=(20, 20))\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Heatmap')\n",
    "    ax[0].imshow(heatmap)\n",
    "\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Distance Field')\n",
    "    ax[1].imshow(distance_field)\n",
    "\n",
    "    ax[2].axis('off')\n",
    "    ax[2].set_title('Angle Field')\n",
    "    ax[2].imshow(get_flow_vis(distance_field, angle_field))\n",
    "\n",
    "    ax[3].axis('off')\n",
    "    ax[3].set_title('Original')\n",
    "    ax[3].imshow(img.permute(1,2,0))\n",
    "    ax[3].scatter(keypoints[:,1],keypoints[:,0], marker=\"o\", color=\"red\", s=3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971fa2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/03/2024 12:46:26 gluefactory.datasets.base_dataset INFO] Creating dataset OxfordParisMiniOneViewJPLDD\n",
      "[11/03/2024 12:46:26 gluefactory.datasets.oxford_paris_mini_1view_jpldd INFO] NUMBER OF IMAGES: 11500\n",
      "[11/03/2024 12:46:26 gluefactory.datasets.oxford_paris_mini_1view_jpldd INFO] KNOWN BATCHSIZE FOR MY SPLIT(train) is 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET OVERALL(NO-SPLIT) IMAGES: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/03/2024 12:46:26 gluefactory.datasets.oxford_paris_mini_1view_jpldd INFO] NUMBER OF IMAGES WITH GT: 11500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['name', 'scales', 'image_size', 'transform', 'original_image_size', 'image', 'padding_mask', 'deeplsd_distance_field', 'deeplsd_angle_field', 'superpoint_heatmap', 'gt_keypoints', 'gt_keypoints_scores'])\n",
      "AF: type: <class 'torch.Tensor'>, shape: torch.Size([600, 600]), min: 0.0, max: 3.1415905952453613\n",
      "DF: type: <class 'torch.Tensor'>, shape: torch.Size([600, 600]), min: 0.0, max: 4.999999046325684\n",
      "KP-HMAP: type: <class 'torch.Tensor'>, shape: torch.Size([600, 600]), min: 0.0, max: 0.7789580821990967, sum: 76.7752685546875\n"
     ]
    }
   ],
   "source": [
    "dset_conf = {\n",
    "    \"reshape\": 600,  # ex. 800\n",
    "    \"multiscale_learning\": {\n",
    "        \"do\": False,\n",
    "        \"scales_list\": [800, 600, 400],\n",
    "        \"scale_selection\": 'round-robin' # random or round-robin\n",
    "    },\n",
    "    \"load_features\": {\n",
    "        \"do\": True,\n",
    "        \"check_exists\": True,\n",
    "        \"point_gt\": {\n",
    "            \"data_keys\": [\"superpoint_heatmap\", \"gt_keypoints\", \"gt_keypoints_scores\"],\n",
    "            \"use_score_heatmap\": True,\n",
    "        },\n",
    "        \"line_gt\": {\n",
    "            \"data_keys\": [\"deeplsd_distance_field\", \"deeplsd_angle_field\"],\n",
    "            \"enforce_threshold\": 5.0,  # Enforce values in distance field to be no greater than this value\n",
    "        },\n",
    "    },\n",
    "    #\"debug\": True\n",
    "}\n",
    "oxpa_2 = get_dataset(\"oxford_paris_mini_1view_jpldd\")(dset_conf)\n",
    "ds = oxpa_2.get_dataset(split=\"train\")\n",
    "\n",
    "# load one test element\n",
    "elem = ds[0]\n",
    "print(f\"Keys: {elem.keys()}\")\n",
    "\n",
    "# print example shapes\n",
    "af = elem[\"deeplsd_angle_field\"]\n",
    "df = elem[\"deeplsd_distance_field\"]\n",
    "hmap = elem[\"superpoint_heatmap\"]\n",
    "\n",
    "print(f\"AF: type: {type(af)}, shape: {af.shape}, min: {torch.min(af)}, max: {torch.max(af)}\")\n",
    "print(f\"DF: type: {type(df)}, shape: {df.shape}, min: {torch.min(df)}, max: {torch.max(df)}\")\n",
    "print(f\"KP-HMAP: type: {type(hmap)}, shape: {hmap.shape}, min: {torch.min(hmap)}, max: {torch.max(hmap)}, sum: {torch.sum(hmap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f5940-32d1-4e04-81f6-45bb4709082e",
   "metadata": {},
   "source": [
    "## Run a model to get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6100068",
   "metadata": {},
   "source": [
    "Define utility for visualisation and example calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "870873b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_visualize(img_data_with_gt, model, device):\n",
    "    img_torch = img_data_with_gt[\"image\"].to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output_model = model({\"image\": img_torch})\n",
    "    jpldd_kpjhm = output_model['keypoint_and_junction_score_map'][0].cpu()\n",
    "    jpldd_af = output_model['line_anglefield'][0].cpu()\n",
    "    jpldd_df = output_model['line_distancefield'][0].cpu()\n",
    "    jpldd_kp = output_model['keypoints'][0].cpu()\n",
    "        \n",
    "    af_gt = img_data_with_gt[\"deeplsd_angle_field\"].cpu()\n",
    "    df_gt = img_data_with_gt[\"deeplsd_distance_field\"].cpu()\n",
    "    hmap_gt = img_data_with_gt[\"superpoint_heatmap\"].cpu()\n",
    "    orig_pt_gt = img_data_with_gt[\"orig_points\"].cpu()\n",
    "    \n",
    "    _, ax = plt.subplots(2, 4, figsize=(20, 8))\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 0].set_title('GT-Heatmap')\n",
    "    ax[0, 0].imshow(hmap_gt)\n",
    "\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[0, 1].set_title('GT-Distance Field')\n",
    "    ax[0, 1].imshow(df_gt)\n",
    "\n",
    "    ax[0, 2].axis('off')\n",
    "    ax[0, 2].set_title('GT-Angle Field')\n",
    "    ax[0, 2].imshow(get_flow_vis(df_gt, af_gt))\n",
    "\n",
    "    ax[0, 3].axis('off')\n",
    "    ax[0, 3].set_title('GT-Original')\n",
    "    ax[0, 3].imshow(img_data_with_gt[\"image\"].permute(1,2,0))\n",
    "    ax[0, 3].scatter(orig_pt_gt[:, 0], orig_pt_gt[:, 1], marker=\"o\", color=\"red\", s=3)\n",
    "    \n",
    "    ax[1, 0].axis('off')\n",
    "    ax[1, 0].set_title('KP&J Heatmap')\n",
    "    ax[1, 0].imshow(jpldd_kpjhm)\n",
    "\n",
    "    ax[1, 1].axis('off')\n",
    "    ax[1, 1].set_title('Distance Field')\n",
    "    ax[1, 1].imshow(jpldd_df)\n",
    "\n",
    "    ax[1, 2].axis('off')\n",
    "    ax[1, 2].set_title('Angle Field')\n",
    "    ax[1, 2].imshow(get_flow_vis(jpldd_df, jpldd_af))\n",
    "\n",
    "    ax[1, 3].axis('off')\n",
    "    ax[1, 3].set_title('Original w. detected kp')\n",
    "    ax[1, 3].imshow(img_torch.cpu().squeeze(0).permute(1,2,0))\n",
    "    ax[1, 3].scatter(jpldd_kp[:, 0], jpldd_kp[:, 1], marker=\"o\", color=\"red\", s=3)\n",
    "    plt.show()\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d5d548e-3099-488b-ae12-336b2321eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load model parameters from checkpoint /local/home/Point-Line/outputs/training/focal_loss_experiments/rk_focal_threshDF_focal/checkpoint_best.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Used: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/03/2024 13:04:17 gluefactory.models.lines.pold2_cnn INFO] Successfully loaded model weights from /local/home/Point-Line/outputs/training/oxparis_100lines_per_img/checkpoint_best.tar\n"
     ]
    }
   ],
   "source": [
    "from gluefactory.models import get_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Device Used: {device}\")\n",
    "\n",
    "DEBUG_DIR = \"tmp_testbed\"\n",
    "jpldd_conf = {\n",
    "    \"name\": \"joint_point_line_extractor\",\n",
    "    \"max_num_keypoints\": 1000,  # setting for training, for eval: -1\n",
    "    \"timeit\": True,  # override timeit: False from BaseModel\n",
    "    \"line_df_decoder_channels\": 32,\n",
    "    \"line_af_decoder_channels\": 32,\n",
    "    \"line_detection\": {\n",
    "        \"do\": True,\n",
    "        \"conf\": {\n",
    "            \"max_point_size\": 1500,\n",
    "            \"min_line_length\": 10,\n",
    "            \"max_line_length\": None,\n",
    "            \"samples\": [8],\n",
    "\n",
    "            \"distance_map\": {\n",
    "                \"max_value\": 5, # this is fixed and only determined by the distance threshold\n",
    "                \"threshold\": 0.5, # lower value gets more lines\n",
    "                \"smooth_threshold\": 0.85, # higher seems to be less strict (i.e. more lines)\n",
    "                \"avg_filter_size\": 13,\n",
    "                \"avg_filter_padding\": 6,\n",
    "                \"avg_filter_stride\": 1,\n",
    "                \"inlier_ratio\": 0.8, # gives artifacts at 0.5 (lower is more lines)\n",
    "                \"max_accepted_mean_value\": 0.4, # be very careful with this. Only small increases can caus\n",
    "            },\n",
    "\n",
    "            \"mlp_conf\": {\n",
    "                \"has_angle_field\": True,\n",
    "                \"has_distance_field\": True, \n",
    "                \"num_bands\": 3,\n",
    "                \"band_width\": 1,\n",
    "                \"num_line_samples\": 30,\n",
    "                \"mlp_hidden_dims\": [256, 128, 128, 64, 32],\n",
    "                \"pred_threshold\": 0.9,\n",
    "                #\"weights\": \"/local/home/Point-Line/outputs/training/pold2_cnn_test/checkpoint_best.tar\",\n",
    "                #\"weights\": \"/local/home/Point-Line/outputs/training/pold2_mlp_gen+train_Bands-3-1_1kimg_60pimg_NEG-Combined/checkpoint_best.tar\",\n",
    "                #\"weights\" : \"/local/home/Point-Line/outputs/training/pold2_cnn_hard_negative/checkpoint_best.tar\"\n",
    "                \"weights\" : \"/local/home/Point-Line/outputs/training/oxparis_100lines_per_img/checkpoint_best.tar\"  \n",
    "                #\"weights\": \"/local/home/Point-Line/outputs/training/pold2_mlp_train_1k_150samp/checkpoint_best.tar\",\n",
    "            },\n",
    "            \"nms\": True,\n",
    "            \"debug\": False,\n",
    "            \"debug_dir\": DEBUG_DIR,\n",
    "        }\n",
    "    },\n",
    "    \"checkpoint\": \"/local/home/Point-Line/outputs/training/focal_loss_experiments/rk_focal_threshDF_focal/checkpoint_best.tar\"\n",
    "    #\"checkpoint\": \"/local/home/rkreft/shared_team_folder/outputs/training/rk_oxparis_focal_hard_gt/checkpoint_best.tar\"\n",
    "    #\"checkpoint\": \"/local/home/rkreft/shared_team_folder/outputs/training/rk_pold2gt_oxparis_base_hard_gt/checkpoint_best.tar\"\n",
    "}\n",
    "jpldd_model_150 = get_model(\"joint_point_line_extractor\")(jpldd_conf).to(device)\n",
    "jpldd_model_150.eval()\n",
    "conf2 = deepcopy(jpldd_conf)\n",
    "conf2[\"line_detection\"][\"conf\"][\"mlp_conf\"][\"num_line_samples\"] = 30\n",
    "conf2[\"line_detection\"][\"conf\"][\"mlp_conf\"][\"weights\"] = \"/local/home/Point-Line/outputs/training/pold2_mlp_train_1k_30samp/checkpoint_best.tar\"\n",
    "#jpldd_model_30 = get_model(\"joint_point_line_extractor\")(conf2).to(device)\n",
    "#jpldd_model_30.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb80b1",
   "metadata": {},
   "source": [
    "## Timings\n",
    "Here we run some fw passes for single images and print timings. Check settings of model and dataset to interpret results correctly. Results are given as mean seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe32f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptor-branch': np.float64(0.0029625892639160156),\n",
      " 'encoder': np.float64(0.0049277544021606445),\n",
      " 'keypoint-and-junction-heatmap': np.float64(0.0012475252151489258),\n",
      " 'keypoint-detection': np.float64(0.0007114410400390625),\n",
      " 'line-af': np.float64(0.005682587623596191),\n",
      " 'line-detection': np.float64(0.09561634063720703),\n",
      " 'line-df': np.float64(0.005682587623596191),\n",
      " 'total-makespan': np.float64(0.11956310272216797)}\n",
      "~FPS: 8.363784288232527 using device cuda\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "rand_idx = random.sample(range(0, len(ds)), 300) \n",
    "\n",
    "for i in rand_idx:\n",
    "    img_torch = ds[i][\"image\"].to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output_model = jpldd_model_150({\"image\": img_torch})\n",
    "\n",
    "timings=jpldd_model_150.get_current_timings(reset=True)\n",
    "pprint(timings)\n",
    "print(f\"~FPS: {1 / (timings['total-makespan'])} using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fac1abc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['keypoint_and_junction_score_map', 'line_anglefield', 'line_distancefield', 'keypoints_raw', 'keypoints', 'keypoint_scores', 'descriptors', 'lines', 'valid_lines'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670efa5",
   "metadata": {},
   "source": [
    "## Comparing JPLDD and DeepLSD lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0905189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    }
   ],
   "source": [
    "# should use 544\n",
    "# looks good with 506\n",
    "img_index = 506\n",
    "deeplsd_net = get_deep_lsd_model()\n",
    "with torch.no_grad():\n",
    "    img_torch = ds[img_index][\"image\"].to(device).unsqueeze(0)\n",
    "    img = (img_torch[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    c_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    gray_img = cv2.cvtColor(c_img, cv2.COLOR_BGR2GRAY)\n",
    "    inputs = {\n",
    "        \"image\": torch.tensor(gray_img, dtype=torch.float, device=device)[\n",
    "            None, None\n",
    "        ]\n",
    "        / 255.0\n",
    "    }\n",
    "    deeplsd_output = deeplsd_net(inputs)\n",
    "    deeplsd_lines = np.array(deeplsd_output[\"lines\"][0]).astype(int)\n",
    "    output_model = jpldd_model_150({\"image\": img_torch})\n",
    "\n",
    "deeplsd_lines_torch = torch.clamp(torch.tensor(deeplsd_lines).cuda(),0,599)\n",
    "keypoints_deeplsd = torch.cat((deeplsd_lines_torch[:,0],deeplsd_lines_torch[:,1]))\n",
    "line_data  = {\n",
    "    \"points\": keypoints_deeplsd,\n",
    "    \"distance_map\": output_model[\"line_distancefield\"][0],\n",
    "    \"angle_map\": output_model[\"line_anglefield\"][0],\n",
    "    \"descriptors\": torch.zeros((keypoints_deeplsd.shape[0],128)).cuda(), \n",
    "}\n",
    "#mlp_filter(self, points: torch.Tensor, indices_image: torch.Tensor, distance_map: torch.Tensor, angle_map: torch.Tensor)\n",
    "#points (torch.Tensor): all keypoints, Shape: (#keypoints , 2)\n",
    "#indices_image (torch.Tensor): Each row contains indices of 2 keypoints (indexing keypoint list \"points\"). Thus each row = 1 line candidate. Shape: (#candidate_lines , 2)\n",
    "#distance_map (torch.Tensor): distance field as in DeepLSD\n",
    "#angle_map (torch.Tensor): angle field as in DeepLSD\n",
    "#img_idx = torch.arange(0,len(deeplsd_lines_torch))\n",
    "#img_idx = torch.vstack((img_idx,len(deeplsd_lines_torch) + img_idx)).transpose(0,1).cuda()\n",
    "#cur_lines = jpldd_model_150.line_extractor.mlp_filter(points=keypoints_deeplsd,indices_image=img_idx,distance_map=output_model[\"line_distancefield\"][0],angle_map=output_model[\"line_anglefield\"][0])\n",
    "cur_lines = jpldd_model_150.line_extractor(line_data)[\"lines\"]\n",
    "#cur_lines = output_model[\"lines\"][0]\n",
    "keypoints_overall = keypoints_deeplsd\n",
    "print(len(cur_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e2337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0701, 95.1586],\n",
       "        [64.1068, 97.1619]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eedb6732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1158f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 78, 568], device='cuda:0'), tensor([ 77, 536], device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_deeplsd[img_idx[1,0]],keypoints_deeplsd[img_idx[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd398ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 78, 568],\n",
       "        [ 77, 536]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplsd_lines_torch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b1291",
   "metadata": {},
   "source": [
    "## 150 samples vs 30 samples (num lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5877d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg lines JPLDD (30 samples): 53\n",
      "Avg lines JPLDD (150 samples): 50\n"
     ]
    }
   ],
   "source": [
    "avg_lines_150 = 0\n",
    "avg_lines_30 = 0\n",
    "num_samples = 500\n",
    "for i in range(num_samples):\n",
    "    with torch.no_grad():\n",
    "        img_torch = ds[i][\"image\"].to(device).unsqueeze(0)\n",
    "        output_model_150 = jpldd_model_150({\"image\": img_torch})\n",
    "        output_model_30 = jpldd_model_30({\"image\": img_torch})\n",
    "    \n",
    "    avg_lines_30 += len(output_model_30[\"lines\"][0])\n",
    "    avg_lines_150 += len(output_model_150[\"lines\"][0])\n",
    "print(f\"Avg lines JPLDD (30 samples): {avg_lines_30//num_samples}\")\n",
    "print(f\"Avg lines JPLDD (150 samples): {avg_lines_150//num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8aff4b",
   "metadata": {},
   "source": [
    "## 30 vs 150 samples (timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38801db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptor-branch': np.float64(0.0016859769821166992),\n",
      " 'encoder': np.float64(0.0053920745849609375),\n",
      " 'keypoint-and-junction-heatmap': np.float64(0.0014081001281738281),\n",
      " 'keypoint-detection': np.float64(0.0008263587951660156),\n",
      " 'line-af': np.float64(0.006181955337524414),\n",
      " 'line-detection': np.float64(0.02024531364440918),\n",
      " 'line-df': np.float64(0.0061833858489990234),\n",
      " 'total-makespan': np.float64(0.044205665588378906)}\n",
      "~FPS: 22.62153474424525 using device cuda\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "jpldd_model_150.get_current_timings(reset=True)\n",
    "num_samples = 500\n",
    "for i in range(num_samples):\n",
    "    img_torch = ds[i][\"image\"].to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output_model = jpldd_model_150({\"image\": img_torch})\n",
    "\n",
    "timings=jpldd_model_150.get_current_timings(reset=True)\n",
    "pprint(timings)\n",
    "print(f\"~FPS: {1 / (timings['total-makespan'])} using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6318fdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptor-branch': np.float64(0.0016865730285644531),\n",
      " 'encoder': np.float64(0.005391359329223633),\n",
      " 'keypoint-and-junction-heatmap': np.float64(0.0014088153839111328),\n",
      " 'keypoint-detection': np.float64(0.0008265972137451172),\n",
      " 'line-af': np.float64(0.006180763244628906),\n",
      " 'line-detection': np.float64(0.020911335945129395),\n",
      " 'line-df': np.float64(0.006183147430419922),\n",
      " 'total-makespan': np.float64(0.0448908805847168)}\n",
      "~FPS: 22.27623933802832 using device cuda\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "jpldd_model_30.get_current_timings(reset=True)\n",
    "num_samples = 500\n",
    "for i in range(num_samples):\n",
    "    img_torch = ds[i][\"image\"].to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output_model = jpldd_model_30({\"image\": img_torch})\n",
    "\n",
    "timings=jpldd_model_30.get_current_timings(reset=True)\n",
    "pprint(timings)\n",
    "print(f\"~FPS: {1 / (timings['total-makespan'])} using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a777e2",
   "metadata": {},
   "source": [
    "Seems like 150 vs 30 samples does not really make a difference regarding time. But using 30 samples makes the model detect a few more lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06266286",
   "metadata": {},
   "source": [
    "# Harris Corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d426c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should use 544\n",
    "# castle image is index 100\n",
    "img_index = 100\n",
    "deeplsd_net = get_deep_lsd_model()\n",
    "with torch.no_grad():\n",
    "    img_torch = ds[img_index][\"image\"].to(device).unsqueeze(0)\n",
    "    img = (img_torch[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    c_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    gray_img = cv2.cvtColor(c_img, cv2.COLOR_BGR2GRAY)\n",
    "    inputs = {\n",
    "            \"image\": torch.tensor(gray_img, dtype=torch.float, device=device)[\n",
    "                None, None\n",
    "            ]\n",
    "            / 255.0\n",
    "        }\n",
    "    deeplsd_output = deeplsd_net(inputs)\n",
    "    output_model = jpldd_model_150({\"image\": img_torch})\n",
    "deeplsd_lines = np.array(deeplsd_output[\"lines\"][0]).astype(int)\n",
    "deeplsd_lines_torch = torch.tensor(deeplsd_lines).cuda()\n",
    "keypoints_deeplsd = torch.cat((deeplsd_lines_torch[:,0],deeplsd_lines_torch[:,1]))\n",
    "dst = cv2.cornerHarris(gray_img,3,3,0.04)\n",
    "dst_norm = np.empty(dst.shape,dtype=np.float32)\n",
    "cv2.normalize(dst,dst_norm,alpha=0,beta=255,norm_type=cv2.NORM_MINMAX)\n",
    "dst_norm_scaled = cv2.convertScaleAbs(dst_norm)\n",
    "keypoints_harris_switched = torch.Tensor(np.transpose(np.where(dst_norm_scaled > 120))).cuda()\n",
    "keypoints_harris = torch.zeros_like(keypoints_harris_switched)\n",
    "keypoints_harris[:,0],keypoints_harris[:,1] = keypoints_harris_switched[:,1],keypoints_harris_switched[:,0]\n",
    "\n",
    "num_orig_keypoints = len(output_model[\"keypoints\"][0])\n",
    "keypoints_overall = torch.cat((output_model[\"keypoints\"][0],keypoints_harris))\n",
    "\n",
    "line_data  = {\n",
    "    \"points\": keypoints_overall,\n",
    "    \"distance_map\": output_model[\"line_distancefield\"][0],\n",
    "    \"angle_map\": output_model[\"line_anglefield\"][0],\n",
    "    \"descriptors\": torch.zeros((keypoints_overall.shape[0],128)).cuda(), \n",
    "}\n",
    "cur_lines = jpldd_model_150.line_extractor(line_data)[\"lines\"]\n",
    "keypoints_deeplsd = keypoints_deeplsd.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51898782",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e239d9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lines_jpldd \u001b[38;5;241m=\u001b[39m cur_lines\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 2\u001b[0m keypoints_overall \u001b[38;5;241m=\u001b[39m \u001b[43mkeypoints_overall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m keypoints_deeplsd \u001b[38;5;241m=\u001b[39m keypoints_deeplsd\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m _, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "lines_jpldd = cur_lines.cpu().numpy()\n",
    "keypoints_overall = keypoints_overall.cpu().numpy()\n",
    "keypoints_deeplsd = keypoints_deeplsd.cpu().numpy()\n",
    "_, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title(\"JPLDD Lines\")\n",
    "#ax[0].scatter(keypoints_overall[:num_orig_keypoints,0],keypoints_overall[:num_orig_keypoints,1],s=5,c=\"y\",label=\"JPLDD Keypoints\")\n",
    "ax[0].scatter(keypoints_overall[:,0],keypoints_overall[:,1],s=3,c=\"r\")\n",
    "ax[0].imshow(ds[img_index][\"image\"].permute(1, 2, 0))\n",
    "ax[0].legend()\n",
    "print(\"Num JPLDD lines\", lines_jpldd.shape[0])\n",
    "for i in range(lines_jpldd.shape[0]):\n",
    "    line = lines_jpldd[i]\n",
    "    x1,y1 = line[0]\n",
    "    x2,y2 = line[1]\n",
    "    ax[0].plot([x1,x2],[y1,y2],'y-')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title(\"DeepLSD Lines\")\n",
    "ax[1].scatter(keypoints_deeplsd[:,0],keypoints_deeplsd[:,1],s=5,c=\"r\")\n",
    "ax[1].imshow(ds[img_index][\"image\"].permute(1, 2, 0))\n",
    "print(\"DeepLSD lines\", deeplsd_lines.shape[0])\n",
    "for i in range(deeplsd_lines.shape[0]):\n",
    "    line = deeplsd_lines[i]\n",
    "    x1,y1 = line[0]\n",
    "    x2,y2 = line[1]\n",
    "    plt.plot([x1,x2],[y1,y2],'y-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44fb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
