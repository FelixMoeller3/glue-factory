# This is a basic config for the merged dataset with combining minidepth and oxparis with multiscale training. I copied the datasets configs(minidepth, oxparis) from the respective base configs
data:
    name: gluefactory.datasets.merge_datasets
    train_batch_size: 4  # prefix must match split
    val_batch_size: 4
    num_workers: 6  # number of workers used by the Dataloader
    prefetch_factor: 8
    inter_dataset_shuffle: True  # if True, all images are shuffled (from all datasets) -> scale selection needs to be random in this case as otherwise datsaets will have always choosing same size
    use_multiscale_learning: True  # if True, we assume that all datasets included use multiscale learning. -> will make the datasets output same size for a batch
    datasets: # Here list datasets with their (file)name. As an example we have Oxparis and Minidepth here
        minidepth:
            name: "gluefactory.datasets.minidepth"
            data_dir: "minidepth"  
            grayscale: False
            reshape: null  
            square_pad: True
            multiscale_learning:
                do: True
                scales_list: [800, 600, 400]
                scale_selection: 'random'
            load_features:
                do: True
                check_exists: True
                point_gt:
                    data_keys: [ "superpoint_heatmap", "gt_keypoints", "gt_keypoints_scores" ]
                    use_score_heatmap: False
                    max_num_keypoints: 63  # this num must match the max num kp at other datasets (take minimum)
                line_gt:
                    data_keys: [ "deeplsd_distance_field", "deeplsd_angle_field" ]
            train_scenes_file_path: "gluefactory/datasets/minidepth_train_scenes.txt"
            val_scenes_file_path: "gluefactory/datasets/minidepth_val_scenes.txt"
        oxparis:
            name: "gluefactory.datasets.oxford_paris_mini_1view_jpldd"
            data_dir: "revisitop1m_POLD2/jpg"
            grayscale: False
            reshape: null
            square_pad: True
            multiscale_learning:
                do: True
                scales_list: [800, 600, 400]
                scale_selection: 'random'
            load_features:
                do: True
                check_exists: True
                point_gt:
                    data_keys: [ "superpoint_heatmap", "gt_keypoints", "gt_keypoints_scores" ]
                    use_score_heatmap: False
                    max_num_keypoints: 63
                line_gt:
                    data_keys: [ "deeplsd_distance_field", "deeplsd_angle_field" ]
            val_size: 500
            train_size: 11500

model:
    name: joint_point_line_extractor
    model_name: "aliked-n16"
    line_neighborhood: 5
    max_num_keypoints: 1000  # setting for training, for eval: -1
    training:
        do: True
        aliked_pretrained: True
        pretrain_kp_decoder: True
        train_descriptors:
            do: True # if train is True, initialize ALIKED Light model form OTF Descriptor GT
            gt_aliked_model: "aliked-n32"
        loss:
            kp_loss_name: "weighted_bce"
            kp_loss_parameters:
                lambda_weighted_bce: 200
                focal_alpha: 0.25
                focal_gamma: 2
            loss_weights:
                line_af_weight: 1
                line_df_weight: 1
                keypoint_weight: 1
                descriptor_loss: 1
    line_detection:
      do: False
    checkpoint: null  # if given load model weights from this checkpoint
    timeit: False  # override timeit: False from BaseModel
train:
    load_experiment: null  # initialize the model from a previous experiment (take weights)
    seed: 7
    epochs: 60
    log_every_iter: 8
    eval_every_iter: 15000 # automatically creates new checkpoint if new best eval metric is reached -> set > #batches to not eval except at the end of each epoch
    save_every_iter: 10000
    test_every_epoch: -1
    optimizer: "adam"
    best_key: "loss/total" # key used to determine best checkpoint and evaluation progress
    lr: 0.0001
    lr_schedule:
        type: 'ReduceLROnPlateau'
        on_epoch: True
        options:
            patience: 5
    keep_last_checkpoints: 5
    submodules: []

